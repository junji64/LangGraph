{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb63acc4",
   "metadata": {},
   "source": [
    "##### References\n",
    "* https://youtu.be/R8KB-Zcynxc?si=0LNzv1q7DugMXUCR\n",
    "* https://youtu.be/ny215UUXbhI?si=TtY8tMmZF9CmGnK2\n",
    "\n",
    "\n",
    "## Building the simplest Graph\n",
    "\n",
    "하나의 엣지(edge)로 연결된 두 개의 노드(node)가 있는 그래프로 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3ca588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langgraph\n",
      "  Obtaining dependency information for langgraph from https://files.pythonhosted.org/packages/bc/6f/ad63ff73f5baade85bf5ebcc190301ab7725e5b60b469859213456f3e730/langgraph-0.1.1-py3-none-any.whl.metadata\n",
      "  Downloading langgraph-0.1.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting langchain-core<0.3,>=0.2 (from langgraph)\n",
      "  Obtaining dependency information for langchain-core<0.3,>=0.2 from https://files.pythonhosted.org/packages/cd/79/319f4898ce837e62d3611ed5da9246d10e3a966bb60fadb54901563867cf/langchain_core-0.2.10-py3-none-any.whl.metadata\n",
      "  Downloading langchain_core-0.2.10-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<0.3,>=0.2->langgraph) (6.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<0.3,>=0.2->langgraph) (1.33)\n",
      "Collecting langsmith<0.2.0,>=0.1.75 (from langchain-core<0.3,>=0.2->langgraph)\n",
      "  Obtaining dependency information for langsmith<0.2.0,>=0.1.75 from https://files.pythonhosted.org/packages/11/4f/1d89e18d542b4545787a806dd8b485b5178781cd6c99e7c79285fd862a34/langsmith-0.1.82-py3-none-any.whl.metadata\n",
      "  Downloading langsmith-0.1.82-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<0.3,>=0.2->langgraph) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<0.3,>=0.2->langgraph) (2.7.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<0.3,>=0.2->langgraph) (8.2.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2->langgraph) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (3.10.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (2.31.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2->langgraph) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2->langgraph) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2->langgraph) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (2023.7.22)\n",
      "Downloading langgraph-0.1.1-py3-none-any.whl (87 kB)\n",
      "   ---------------------------------------- 0.0/87.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 87.3/87.3 kB ? eta 0:00:00\n",
      "Downloading langchain_core-0.2.10-py3-none-any.whl (332 kB)\n",
      "   ---------------------------------------- 0.0/332.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 332.8/332.8 kB 20.2 MB/s eta 0:00:00\n",
      "Downloading langsmith-0.1.82-py3-none-any.whl (127 kB)\n",
      "   ---------------------------------------- 0.0/127.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 127.4/127.4 kB 7.8 MB/s eta 0:00:00\n",
      "Installing collected packages: langsmith, langchain-core, langgraph\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.51\n",
      "    Uninstalling langsmith-0.1.51:\n",
      "      Successfully uninstalled langsmith-0.1.51\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.46\n",
      "    Uninstalling langchain-core-0.1.46:\n",
      "      Successfully uninstalled langchain-core-0.1.46\n",
      "Successfully installed langchain-core-0.2.10 langgraph-0.1.1 langsmith-0.1.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script langsmith.exe is installed in 'C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.1.16 requires langchain-core<0.2.0,>=0.1.42, but you have langchain-core 0.2.10 which is incompatible.\n",
      "langchain-community 0.0.34 requires langchain-core<0.2.0,>=0.1.45, but you have langchain-core 0.2.10 which is incompatible.\n",
      "langchain-text-splitters 0.0.1 requires langchain-core<0.2.0,>=0.1.28, but you have langchain-core 0.2.10 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "#!pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d46c0a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Ipython.display import Image\n",
    "# Image(url=\"https://pbs.twimg.com/media/GGcD9z2W4AAeSHE?format=jpg&name=small\", width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70da5043",
   "metadata": {},
   "source": [
    "<img src=\"https://pbs.twimg.com/media/GGcD9z2W4AAeSHE?format=jpg&name=small\" width=\"400\">\n",
    "\n",
    "노드는 필요에 따라 호출할 수 있는 함수처럼 작동합니다. 우리의 경우 node_1이 시작점이고 node_2가 종료점입니다.\n",
    "<img src=\"langgraph-2.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aee883b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    return input_1 + \" Hi \"\n",
    "\n",
    "def function_2(input_2):\n",
    "    return input_2 + \"there\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8573f855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"node_1\", function_1)\n",
    "workflow.add_node(\"node_2\", function_2)\n",
    "workflow.add_edge('node_1', 'node_2')\n",
    "workflow.set_entry_point(\"node_1\")\n",
    "workflow.set_finish_point(\"node_2\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69df96b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Hi there'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ef68286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent': 'Hello! How can I assist you today?'\n",
      "======\n",
      "Output from node 'node_2': 'Agent Says: Hello! How can I assist you today?'\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "input='Hello'\n",
    "for output in app.stream(input):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}': '{value}'\")\n",
    "    print(\"======\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b17d80a",
   "metadata": {},
   "source": [
    "보시다시피 노드를 함수처럼 실행하고 그로부터 일부 값을 반환할 수 있습니다.\n",
    "\n",
    "### Adding LLM Call\n",
    "\n",
    "이제 첫 번째 노드를 Open AI model을 호출할 수 있는 \"agent\"로 만들어 보겠습니다. 우리는 langchain을 사용하여 이 호출을 쉽게 만들 수 있습니다.\n",
    "\n",
    "<img src=\"langgraph-3.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9af63d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (0.1.16)\n",
      "Collecting langchain_openai\n",
      "  Obtaining dependency information for langchain_openai from https://files.pythonhosted.org/packages/8c/a7/79f6c6495d6eeb88d4c435cba4fe2e4da5a3d5f608793dc4753033ef2c73/langchain_openai-0.1.10-py3-none-any.whl.metadata\n",
      "  Downloading langchain_openai-0.1.10-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (3.8.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (0.0.34)\n",
      "Collecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
      "  Obtaining dependency information for langchain-core<0.2.0,>=0.1.42 from https://files.pythonhosted.org/packages/43/8b/48b7e6de9041d2b33d5108e154b82d1bd6c47cc68f0e44cb4fcdaccf5ec7/langchain_core-0.1.52-py3-none-any.whl.metadata\n",
      "  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (0.1.82)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (2.7.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (8.2.3)\n",
      "INFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain_openai\n",
      "  Obtaining dependency information for langchain_openai from https://files.pythonhosted.org/packages/15/bb/e8f080c8408673609f15436237ec8f9d8c39ab67986807d0ca2663acd7a0/langchain_openai-0.1.9-py3-none-any.whl.metadata\n",
      "  Downloading langchain_openai-0.1.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Obtaining dependency information for langchain_openai from https://files.pythonhosted.org/packages/95/ac/ec2eaa55a7eea519b17a7687e5bf1387dd313f41cf19750f9cfb96db44f9/langchain_openai-0.1.8-py3-none-any.whl.metadata\n",
      "  Downloading langchain_openai-0.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Obtaining dependency information for langchain_openai from https://files.pythonhosted.org/packages/10/27/43f785670a340363fe46e2893a80293fa12448584f19ba4f9e6d03673552/langchain_openai-0.1.7-py3-none-any.whl.metadata\n",
      "  Downloading langchain_openai-0.1.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting openai<2.0.0,>=1.24.0 (from langchain_openai)\n",
      "  Obtaining dependency information for openai<2.0.0,>=1.24.0 from https://files.pythonhosted.org/packages/25/ae/0c6dfe9211f04869d39b1930eec4e4c62cf5fc553fc102b8835e7ecf3375/openai-1.35.4-py3-none-any.whl.metadata\n",
      "  Downloading openai-1.35.4-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
      "  Obtaining dependency information for tiktoken<1,>=0.7 from https://files.pythonhosted.org/packages/b1/10/c04b4ff592a5f46b28ebf4c2353f735c02ae7f0ce1b165d00748ced6467e/tiktoken-0.7.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tiktoken-0.7.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<0.2.0,>=0.1.42->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.11.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2022.7.9)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.24.0->langchain_openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n",
      "Downloading langchain_openai-0.1.7-py3-none-any.whl (34 kB)\n",
      "Downloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
      "   ---------------------------------------- 0.0/302.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 302.9/302.9 kB 9.1 MB/s eta 0:00:00\n",
      "Downloading openai-1.35.4-py3-none-any.whl (327 kB)\n",
      "   ---------------------------------------- 0.0/327.4 kB ? eta -:--:--\n",
      "   -------------------------------------- - 317.4/327.4 kB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 327.4/327.4 kB 6.7 MB/s eta 0:00:00\n",
      "Downloading tiktoken-0.7.0-cp311-cp311-win_amd64.whl (799 kB)\n",
      "   ---------------------------------------- 0.0/799.0 kB ? eta -:--:--\n",
      "   ---------------------- ----------------- 450.6/799.0 kB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 799.0/799.0 kB 8.4 MB/s eta 0:00:00\n",
      "Installing collected packages: tiktoken, openai, langchain-core, langchain_openai\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.6.0\n",
      "    Uninstalling tiktoken-0.6.0:\n",
      "      Successfully uninstalled tiktoken-0.6.0\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.23.6\n",
      "    Uninstalling openai-1.23.6:\n",
      "      Successfully uninstalled openai-1.23.6\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.2.10\n",
      "    Uninstalling langchain-core-0.2.10:\n",
      "      Successfully uninstalled langchain-core-0.2.10\n",
      "Successfully installed langchain-core-0.1.52 langchain_openai-0.1.7 openai-1.35.4 tiktoken-0.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script openai.exe is installed in 'C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langgraph 0.1.1 requires langchain-core<0.3,>=0.2, but you have langchain-core 0.1.52 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "#!pip install langchain langchain_openai\n",
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75468928",
   "metadata": {},
   "source": [
    "LangChain에서 ChatOpenAI 모델에 대한 일반적인 호출은 다음과 같이 수행됩니다.\n",
    "\n",
    "먼저 OpenAI용 API 키를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f142511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a01184af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0b83c275-b924-4f4a-851a-9d68aef226dd-0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "model.invoke('Hey there')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc6716",
   "metadata": {},
   "source": [
    "AI 응답만 보고 싶다면 다음을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6758fcce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke('Hey there').content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6689ea1e",
   "metadata": {},
   "source": [
    "좋습니다! 이를 염두에 두고 사용자 질문을 모델에 보낼 수 있도록 위의 function_1을 변경해 보겠습니다. 그런 다음 이 응답을 function_2로 보내면 짧은 문자열이 추가되어 사용자에게 반환됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22060623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    response = model.invoke(input_1)\n",
    "    return response.content\n",
    "\n",
    "def function_2(input_2):\n",
    "    return \"Agent Says: \" + input_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff61b542",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = Graph()\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"node_2\", function_2)\n",
    "workflow.add_edge('agent', 'node_2')\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.set_finish_point(\"node_2\")\n",
    "app=workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21724f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agent Says: Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\"Hey there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6825133f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent': 'Hello! How can I assist you today?'\n",
      "====\n",
      "Output from node 'node_2': 'Agent Says: Hello! How can I assist you today?'\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "input = 'Hey there'\n",
    "for output in app.stream(input):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}': '{value}'\")\n",
    "    print(\"====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47adfb9f",
   "metadata": {},
   "source": [
    "### First functional Agent App - City Temperature\n",
    "\n",
    "<img src=\"langgraph-4.png\" width=\"400\">\n",
    "\n",
    "#### Step 1: Parse the city mentioned\n",
    "\n",
    "사용자가 query에서 언급한 도시를 추출해 보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fab689fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    complete_query = \"Your task is to provide only the city name based on the user query. \\\n",
    "        Nothing more, just the city name mentioned. Following is the user query: \" + input_1\n",
    "    response = model.invoke(complete_query)\n",
    "    return response.content\n",
    "\n",
    "def function_2(input_2):\n",
    "    return \"Agent Says: \" + input_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0092dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Langchain graph\n",
    "workflow = Graph()\n",
    "\n",
    "#calling node 1 as agent\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"node_2\", function_2)\n",
    "\n",
    "workflow.add_edge('agent', 'node_2')\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.set_finish_point(\"node_2\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b3c081e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agent Says: Las Vegas'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\"What's the temperature in Las Vegas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038f464d",
   "metadata": {},
   "source": [
    "#### Step 2: Adding a weather API call\n",
    "function_2가 도시 이름을 가져와 해당 도시의 날씨를 제공하도록 하려면 어떻게 해야 할까요?\n",
    "\n",
    "우리는 Open Weather Map이 LangChain에 통합되어 있다는 것을 알고 있습니다.\n",
    "\n",
    "pyown 을 설치하고 [Open Weather Map 웹사이트](https://openweathermap.org/) 에서 API 키를 생성한 다음, 아래 셀을 실행하여 특정 도시의 날씨를 가져와야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d85eb2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyowm\n",
      "  Obtaining dependency information for pyowm from https://files.pythonhosted.org/packages/6e/88/1279817aa7f5988a2ff42a6755fd371f3c1806aca377cb63b3d16684b174/pyowm-3.3.0-py3-none-any.whl.metadata\n",
      "  Downloading pyowm-3.3.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyowm) (2.31.0)\n",
      "Collecting geojson<3,>=2.3.0 (from pyowm)\n",
      "  Obtaining dependency information for geojson<3,>=2.3.0 from https://files.pythonhosted.org/packages/e4/8d/9e28e9af95739e6d2d2f8d4bef0b3432da40b7c3588fbad4298c1be09e48/geojson-2.5.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading geojson-2.5.0-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: PySocks<2,>=1.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyowm) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.20.0->pyowm) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.20.0->pyowm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.20.0->pyowm) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.20.0->pyowm) (2023.7.22)\n",
      "Downloading pyowm-3.3.0-py3-none-any.whl (4.5 MB)\n",
      "   ---------------------------------------- 0.0/4.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/4.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/4.5 MB 919.0 kB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.3/4.5 MB 2.4 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.6/4.5 MB 3.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.7/4.5 MB 3.8 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.7/4.5 MB 3.8 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.8/4.5 MB 2.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.0/4.5 MB 2.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.4/4.5 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.8/4.5 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.4/4.5 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 2.9/4.5 MB 5.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 3.3/4.5 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.6/4.5 MB 5.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 4.3/4.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.5/4.5 MB 6.5 MB/s eta 0:00:00\n",
      "Downloading geojson-2.5.0-py2.py3-none-any.whl (14 kB)\n",
      "Installing collected packages: geojson, pyowm\n",
      "Successfully installed geojson-2.5.0 pyowm-3.3.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install pyowm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1970496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import OpenWeatherMapAPIWrapper\n",
    "load_dotenv()\n",
    "os.environ[\"openweathermap_api_key\"] = os.environ.get(\"OPENWEATHERMAP_API_KEY\")\n",
    "\n",
    "weather = OpenWeatherMapAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cdb03059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Las Vegas, the current weather is as follows:\n",
      "Detailed status: clear sky\n",
      "Wind speed: 6.69 m/s, direction: 170°\n",
      "Humidity: 23%\n",
      "Temperature: \n",
      "  - Current: 37.95°C\n",
      "  - High: 39.69°C\n",
      "  - Low: 36.01°C\n",
      "  - Feels like: 37.3°C\n",
      "Rain: {}\n",
      "Heat index: None\n",
      "Cloud cover: 0%\n"
     ]
    }
   ],
   "source": [
    "weather_data = weather.run(\"Las Vegas\")\n",
    "print(weather_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce2db9",
   "metadata": {},
   "source": [
    "이제 이를 function_2에 통합하고 workflow에서 \"node_2\" 대신 \"tool\" 또는 \"weather_agent\"로 function_2를 호출해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8819926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    complete_query = \"Your task is to provide only the city name based on the user query. \\\n",
    "    Nothing more, just the city name mentioned. Following is the user query: \" + input_1\n",
    "    response = model.invoke(complete_query)\n",
    "    return response.content\n",
    "\n",
    "def function_2(input_2):\n",
    "    weather_data = weather.run(input_2)\n",
    "    return weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98af5b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "workflow = Graph()\n",
    "workflow.add_node('agent', function_1)\n",
    "workflow.add_node('tool', function_2)\n",
    "workflow.add_edge('agent', 'tool')\n",
    "workflow.set_entry_point('agent')\n",
    "workflow.set_finish_point('tool')\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb081984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Las Vegas, the current weather is as follows:\\nDetailed status: clear sky\\nWind speed: 6.69 m/s, direction: 170°\\nHumidity: 23%\\nTemperature: \\n  - Current: 37.95°C\\n  - High: 39.69°C\\n  - Low: 36.01°C\\n  - Feels like: 37.3°C\\nRain: {}\\nHeat index: None\\nCloud cover: 0%'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\"What's the temperature in Las Vegas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4537f02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent': 'Las Vegas'\n",
      "====\n",
      "Output from node 'tool': 'In Las Vegas, the current weather is as follows:\n",
      "Detailed status: clear sky\n",
      "Wind speed: 6.69 m/s, direction: 170°\n",
      "Humidity: 23%\n",
      "Temperature: \n",
      "  - Current: 37.95°C\n",
      "  - High: 39.69°C\n",
      "  - Low: 36.01°C\n",
      "  - Feels like: 37.3°C\n",
      "Rain: {}\n",
      "Heat index: None\n",
      "Cloud cover: 0%'\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "input = \"What's the temperature in Las Vegas\"\n",
    "for output in app.stream(input):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}': '{value}'\")\n",
    "    print('====')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee0a60",
   "metadata": {},
   "source": [
    "#### Step 3 Adding another LLM Call to filter results\n",
    "온도만 원한다면 어떻게 해야할까요? 현재 설정에서는 전체 일기 예보를 제공합니다.\n",
    "\n",
    "데이터를 필터링하기 위해 또 다른 LLM 호출을 할 수 있습니다.\n",
    "\n",
    "<img src=\"langgraph-5.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b86a0eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_3(input_3):\n",
    "    complete_query = \"Your task is to provide info concisely based on the user query. Following is the user query: \" + \"user input\"\n",
    "    response = model.invoke(complete_query)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a112231",
   "metadata": {},
   "source": [
    "그러나 문제는 node_2에서 \"user_input\"을 알 수 없다는 것입니다.\n",
    "\n",
    "첫 번째 node 에서 마지막 node 까지 \"user_input\"을 전달할 수 있나요?\n",
    "\n",
    "예, dictionary 을 사용하여 node 간에 전달할 수 있습니다(list 만 사용할 수도 있지만, dictionary 를 사용하면 좀 더 쉽습니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "515c7d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign AgentState as an empty dict\n",
    "AgentState = {}\n",
    "\n",
    "# messages key will be assigned as an empty array. We will append new messages as we pass along nodes. \n",
    "AgentState[\"messages\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6cf54b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': []}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AgentState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34318f7d",
   "metadata": {},
   "source": [
    "우리의 목표는 이 상태(State)를 다음과 같이 채우는 것입니다: \n",
    "```\n",
    "{'messages': [HumanMessage, AIMessage, ...]]}\n",
    "```\n",
    "또한 이제 새 AgentState 에 따라 정보를 전달하도록 함수를 수정해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1c864a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(state):\n",
    "    messages = state['messages']\n",
    "    user_input = messages[-1]\n",
    "    complete_query = \"Your task is to provide only the city name based on the user query. \\\n",
    "                    Nothing more, just the city name mentioned. Following is the user query: \" + user_input\n",
    "    response = model.invoke(complete_query)\n",
    "    state['messages'].append(response.content) # appending AIMessage response to the AgentState\n",
    "    return state\n",
    "\n",
    "def function_2(state):\n",
    "    messages = state['messages']\n",
    "    agent_response = messages[-1]\n",
    "    weather = OpenWeatherMapAPIWrapper()\n",
    "    weather_data = weather.run(agent_response)\n",
    "    state['messages'].append(weather_data)\n",
    "    return state\n",
    "\n",
    "def function_3(state):\n",
    "    messages = state['messages']\n",
    "    user_input = messages[0]\n",
    "    available_info = messages[-1]\n",
    "    agent2_query = \"Your task is to provide info concisely based on the user query and the available information from the internet. \\\n",
    "                        Following is the user query: \" + user_input + \" Available information: \" + available_info\n",
    "    response = model.invoke(agent2_query)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43c1a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"tool\", function_2)\n",
    "workflow.add_node(\"responder\", function_3)\n",
    "\n",
    "workflow.add_edge('agent', 'tool')\n",
    "workflow.add_edge('tool', 'responder')\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.set_finish_point(\"responder\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "125ca4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature in Las Vegas is 36.7°C with clear skies and a low humidity of 27%.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"messages\": [\"what is the temperature in Las Vegas\"]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2cd87a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent': '{'messages': ['what is the temperature in  Las Vegas', 'Las Vegas']}'\n",
      "====\n",
      "Output from node 'tool': '{'messages': ['what is the temperature in  Las Vegas', 'Las Vegas', 'In Las Vegas, the current weather is as follows:\\nDetailed status: clear sky\\nWind speed: 1.34 m/s, direction: 267°\\nHumidity: 27%\\nTemperature: \\n  - Current: 36.7°C\\n  - High: 37.28°C\\n  - Low: 36.18°C\\n  - Feels like: 36.48°C\\nRain: {}\\nHeat index: None\\nCloud cover: 8%']}'\n",
      "====\n",
      "Output from node 'responder': 'The current temperature in Las Vegas is 36.7°C with clear skies, a low humidity of 27%, and a light wind speed of 1.34 m/s.'\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "input = {\"messages\": [\"what is the temperature in  Las Vegas\"]}\n",
    "for output in app.stream(input):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}': '{value}'\")\n",
    "    print(\"====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c11f4e0",
   "metadata": {},
   "source": [
    "array 에 많은 덧붙임(appending)이 진행되고 있음을 알 수 있으므로, 다음을 사용하여 좀 더 쉽게 만들 수 있습니다:\n",
    "\n",
    "```python\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "```\n",
    "기본적으로 이전에 본 것처럼 State dictionary 를 만들고, 다음을 수행할 때 새 메시지가 메시지 배열에 추가되는지 확인합니다.\n",
    "\n",
    "```python\n",
    "{\"messages\": [new_array_element]}\n",
    "```\n",
    "\n",
    "또한 우리 앱이 \"how are you?\"와 같은 간단한 질문에 답할 수 없다는 것도 알고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a93128e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am an AI assistant, so I don't have feelings, but thank you for asking. In Istanbul, the current weather is clear with a temperature of 22.89°C, a wind speed of 7.72 m/s, and 61% humidity.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"messages\": [\"How are you?\"]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86302e12",
   "metadata": {},
   "source": [
    "이는 우리가 항상 도시를 분석한 다음 날씨를 찾고 싶기 때문입니다.\n",
    "\n",
    "사용자에게 단순히 응답하는 것이 아니라, 필요한 경우에만 도구를 사용하도록 하여 에이전트를 더 똑똑하게 만들 수 있습니다.\n",
    "\n",
    "LangGraph를 수행할 수 있는 방법은 다음과 같습니다.\n",
    "\n",
    "1. tool 를 agent 에 바인딩\n",
    "2. tool 를 호출할지 여부를 선택할 수 있는 옵션으로 agent에 conditianal edge 를 추가합니다.\n",
    "3. tool 를 호출할 시기와 같이 conditianal edge에 대한 기준을 정의합니다. 이에 대한 함수를 정의하겠습니다.\n",
    "\n",
    "위의 몇 가지 셀에서 언급한 AgentState 정의부터 시작해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8f7b0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1c398f",
   "metadata": {},
   "source": [
    "tool을 에이전트(LLM 모델)에 바인딩하는 것이 langchain에서 쉽게 만들어집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f92da8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_community.tools.openweathermap import OpenWeatherMapQueryRun\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "tools = [OpenWeatherMapQueryRun()]\n",
    "\n",
    "model = ChatOpenAI(temperature=0, streaming=True)\n",
    "functions = [convert_to_openai_function(t) for t in tools]\n",
    "model = model.bind_functions(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18962da",
   "metadata": {},
   "source": [
    "수정된 function_1은 이제 아래와 같습니다. 그 이유는 인간의 메시지를 state로 전달하고, 응답을 state에 추가하기 때문입니다. 또한 이제 agent 에는 사용할 수 있는 tool 이 바인딩되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29286431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(state):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e7a61",
   "metadata": {},
   "source": [
    "function_2의 경우 tool 을 설정하고 호출하기를 원합니다. ToolInvocation을 사용하고, ToolExecuter로 실행하면 LangChain 에서 tool 을 쉽게 호출할 수 있습니다. 그런 다음 agent(node_1)가 tool 이 사용되었고, tool 의 응답이 가용하다는 것을 알 수 있도록, FunctionMessage 로 다시 응답합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d96138dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolInvocation\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "def function_2(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1] # this has the query we need to send to the tool provided by the agent\n",
    "\n",
    "    parsed_tool_input = json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"])\n",
    "\n",
    "    # We construct an ToolInvocation from the function_call and pass in the tool name and the expected str input for OpenWeatherMap tool\n",
    "    action = ToolInvocation(\n",
    "        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "        tool_input=parsed_tool_input['__arg1'],\n",
    "    )\n",
    "    \n",
    "    # We call the tool_executor and get back a response\n",
    "    response = tool_executor.invoke(action)\n",
    "\n",
    "    # We use the response to create a FunctionMessage\n",
    "    function_message = FunctionMessage(content=str(response), name=action.tool)\n",
    "\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [function_message]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e58bb4",
   "metadata": {},
   "source": [
    "마지막으로, 어떤 방향(tool 또는 사용자 응답)으로 가야 할지 파악하는 데 도움이 되는 conditional edge 에 대한 함수를 정의합니다.\n",
    "\n",
    "우리는 tool 이름으로 function_call을 만들기 위한 additional_kwargs 를 가지고 있는 LangChain의 agent(LLM) 응답으로부터 이점을 얻을 수 있습니다.\n",
    "\n",
    "따라서 우리의 논리는 additional_kwargs 에서 function_call을 사용할 수 있으면 tool 을 호출하고, 그렇지 않으면 논의를 종료하고 사용자에게 다시 응답하는 것입니다.\n",
    "\n",
    "<img src=\"langgraph-6.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9b78e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def where_to_go(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    if \"function_call\" in last_message.additional_kwargs:\n",
    "        return \"continue\"\n",
    "    else:\n",
    "        return \"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43782915",
   "metadata": {},
   "source": [
    "이제 위의 모든 변경 사항을 적용하여 LangGraph 앱이 아래와 같이 수정되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "be4d022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langgraph.graph import Graph, END\n",
    "\n",
    "# workflow = Graph()\n",
    "\n",
    "# Or you could import StateGraph and pass AgentState to it\n",
    "from langgraph.graph import StateGraph, END\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"tool\", function_2)\n",
    "\n",
    "# The conditional edge requires the following info below.\n",
    "# First, we define the start node. We use `agent`.\n",
    "# This means these are the edges taken after the `agent` node is called.\n",
    "# Next, we pass in the function that will determine which node is called next, in our case where_to_go().\n",
    "\n",
    "workflow.add_conditional_edges(\"agent\", where_to_go,{   # Based on the return from where_to_go\n",
    "                                                        # If return is \"continue\" then we call the tool node.\n",
    "                                                        \"continue\": \"tool\",\n",
    "                                                        # Otherwise we finish. END is a special node marking that the graph should finish.\n",
    "                                                        \"end\": END\n",
    "                                                    }\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that if `tool` is called, then it has to call the 'agent' next. \n",
    "workflow.add_edge('tool', 'agent')\n",
    "\n",
    "# Basically, agent node has the option to call a tool node based on a condition, \n",
    "# whereas tool node must call the agent in all cases based on this setup.\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04ce6ae",
   "metadata": {},
   "source": [
    "또한 langchain에서 사용할 수 있는 HumanMessage 구성 요소를 사용하여 첫 번째 메시지를 전달하므로 AIMessage 및 FunctionMessage와 쉽게 구별할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4c5cf444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the temperature in las vegas'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"__arg1\":\"Las Vegas\"}', 'name': 'open_weather_map'}}, response_metadata={'finish_reason': 'function_call'}, id='run-f0a8b6c2-4c14-4740-8f82-13656a0677e4-0'),\n",
       "  FunctionMessage(content='In Las Vegas, the current weather is as follows:\\nDetailed status: clear sky\\nWind speed: 4.63 m/s, direction: 180°\\nHumidity: 24%\\nTemperature: \\n  - Current: 37.24°C\\n  - High: 38.38°C\\n  - Low: 36.01°C\\n  - Feels like: 36.54°C\\nRain: {}\\nHeat index: None\\nCloud cover: 0%', name='open_weather_map'),\n",
       "  AIMessage(content='The current temperature in Las Vegas is 37.24°C.', response_metadata={'finish_reason': 'stop'}, id='run-57f56fb8-303f-4ce7-a169-669f732d643b-0')]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"what is the temperature in las vegas\")]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09ff6e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent': '{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"__arg1\":\"Las Vegas\"}', 'name': 'open_weather_map'}}, response_metadata={'finish_reason': 'function_call'}, id='run-4c894e29-d3a5-4d4b-828e-635762622960-0')]}'\n",
      "=======\n",
      "Output from node 'tool': '{'messages': [FunctionMessage(content='In Las Vegas, the current weather is as follows:\\nDetailed status: clear sky\\nWind speed: 4.63 m/s, direction: 180°\\nHumidity: 24%\\nTemperature: \\n  - Current: 37.24°C\\n  - High: 38.38°C\\n  - Low: 36.01°C\\n  - Feels like: 36.54°C\\nRain: {}\\nHeat index: None\\nCloud cover: 0%', name='open_weather_map')]}'\n",
      "=======\n",
      "Output from node 'agent': '{'messages': [AIMessage(content='The current temperature in Las Vegas is 37.24°C.', response_metadata={'finish_reason': 'stop'}, id='run-eada6116-159d-4c33-b304-f5f046aa91df-0')]}'\n",
      "=======\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"what is the temperature in las vegas\")]}\n",
    "for output in app.stream(inputs):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}': '{value}'\")\n",
    "    print(\"=======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "15e4133d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm just a computer program, so I don't have feelings, but I'm here to help you with any questions or tasks you have. How can I assist you today?\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"how are you?\")]}\n",
    "app.invoke(inputs)['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "37325dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature in Seoul is 30.74°C.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"what is the temperature in Seoul\")]}\n",
    "app.invoke(inputs)['messages'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f9fc27",
   "metadata": {},
   "source": [
    "이를 통해 우리가 LangGraph 앱을 구축한 방법과 다양한 LangChain 구성요소를 사용한 이유를 잘 이해할 수 있기를 바랍니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
