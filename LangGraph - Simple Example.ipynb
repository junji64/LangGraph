{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb63acc4",
   "metadata": {},
   "source": [
    "##### References\n",
    "* https://youtu.be/R8KB-Zcynxc?si=0LNzv1q7DugMXUCR\n",
    "* https://youtu.be/ny215UUXbhI?si=TtY8tMmZF9CmGnK2\n",
    "\n",
    "\n",
    "## Building the simplest Graph\n",
    "\n",
    "하나의 엣지(edge)로 연결된 두 개의 노드(node)가 있는 그래프로 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d46c0a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Ipython.display import Image\n",
    "# Image(url=\"https://pbs.twimg.com/media/GGcD9z2W4AAeSHE?format=jpg&name=small\", width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70da5043",
   "metadata": {},
   "source": [
    "<img src=\"https://pbs.twimg.com/media/GGcD9z2W4AAeSHE?format=jpg&name=small\" width=\"400\">\n",
    "\n",
    "노드는 필요에 따라 호출할 수 있는 함수처럼 작동합니다. 우리의 경우 node_1이 시작점이고 node_2가 종료점입니다.\n",
    "<img src=\"langgraph-2.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aee883b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    return input_1 + \" Hi \"\n",
    "\n",
    "def function_2(input_2):\n",
    "    return input_2 + \"there\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8573f855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"node_1\", function_1)\n",
    "workflow.add_node(\"node_2\", function_2)\n",
    "workflow.add_edge('node_1', 'node_2')\n",
    "workflow.set_entry_point(\"node_1\")\n",
    "workflow.set_finish_point(\"node_2\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69df96b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Hi there'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ef68286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent': 'Hello! How can I assist you today?'\n",
      "======\n",
      "Output from node 'node_2': 'Agent Says: Hello! How can I assist you today?'\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "input='Hello'\n",
    "for output in app.stream(input):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}': '{value}'\")\n",
    "    print(\"======\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b17d80a",
   "metadata": {},
   "source": [
    "보시다시피 노드를 함수처럼 실행하고 그로부터 일부 값을 반환할 수 있습니다.\n",
    "\n",
    "### Adding LLM Call\n",
    "\n",
    "이제 첫 번째 노드를 Open AI model을 호출할 수 있는 \"agent\"로 만들어 보겠습니다. 우리는 langchain을 사용하여 이 호출을 쉽게 만들 수 있습니다.\n",
    "\n",
    "<img src=\"langgraph-3.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af63d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pip install langchain langchain_openai\n",
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75468928",
   "metadata": {},
   "source": [
    "LangChain에서 ChatOpenAI 모델에 대한 일반적인 호출은 다음과 같이 수행됩니다.\n",
    "\n",
    "먼저 OpenAI용 API 키를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f142511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a01184af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0b83c275-b924-4f4a-851a-9d68aef226dd-0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "model.invoke('Hey there')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc6716",
   "metadata": {},
   "source": [
    "AI 응답만 보고 싶다면 다음을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6758fcce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke('Hey there').content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6689ea1e",
   "metadata": {},
   "source": [
    "좋습니다! 이를 염두에 두고 사용자 질문을 모델에 보낼 수 있도록 위의 function_1을 변경해 보겠습니다. 그런 다음 이 응답을 function_2로 보내면 짧은 문자열이 추가되어 사용자에게 반환됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22060623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    response = model.invoke(input_1)\n",
    "    return response.content\n",
    "\n",
    "def function_2(input_2):\n",
    "    return \"Agent Says: \" + input_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff61b542",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = Graph()\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"node_2\", function_2)\n",
    "workflow.add_edge('agent', 'node_2')\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.set_finish_point(\"node_2\")\n",
    "app=workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21724f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agent Says: Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\"Hey there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6825133f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent': 'Hello! How can I assist you today?'\n",
      "====\n",
      "Output from node 'node_2': 'Agent Says: Hello! How can I assist you today?'\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "input = 'Hey there'\n",
    "for output in app.stream(input):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}': '{value}'\")\n",
    "    print(\"====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47adfb9f",
   "metadata": {},
   "source": [
    "### First functional Agent App - City Temperature\n",
    "\n",
    "<img src=\"langgraph-4.png\" width=\"400\">\n",
    "\n",
    "#### Step 1: Parse the city mentioned\n",
    "\n",
    "사용자가 query에서 언급한 도시를 추출해 보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fab689fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    complete_query = \"Your task is to provide only the city name based on the user query. \\\n",
    "        Nothing more, just the city name mentioned. Following is the user query: \" + input_1\n",
    "    response = model.invoke(complete_query)\n",
    "    return response.content\n",
    "\n",
    "def function_2(input_2):\n",
    "    return \"Agent Says: \" + input_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0092dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Langchain graph\n",
    "workflow = Graph()\n",
    "\n",
    "#calling node 1 as agent\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"node_2\", function_2)\n",
    "\n",
    "workflow.add_edge('agent', 'node_2')\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.set_finish_point(\"node_2\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b3c081e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agent Says: Las Vegas'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\"What's the temperature in Las Vegas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038f464d",
   "metadata": {},
   "source": [
    "#### Step 2: Adding a weather API call\n",
    "function_2가 도시 이름을 가져와 해당 도시의 날씨를 제공하도록 하려면 어떻게 해야 할까요?\n",
    "\n",
    "우리는 Open Weather Map이 LangChain에 통합되어 있다는 것을 알고 있습니다.\n",
    "\n",
    "pyown 을 설치하고 [Open Weather Map 웹사이트](https://openweathermap.org/) 에서 API 키를 생성한 다음, 아래 셀을 실행하여 특정 도시의 날씨를 가져와야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d85eb2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pip install pyowm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1970496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import OpenWeatherMapAPIWrapper\n",
    "load_dotenv()\n",
    "os.environ[\"openweathermap_api_key\"] = os.environ.get(\"OPENWEATHERMAP_API_KEY\")\n",
    "\n",
    "weather = OpenWeatherMapAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cdb03059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Las Vegas, the current weather is as follows:\n",
      "Detailed status: clear sky\n",
      "Wind speed: 6.69 m/s, direction: 170°\n",
      "Humidity: 23%\n",
      "Temperature: \n",
      "  - Current: 37.95°C\n",
      "  - High: 39.69°C\n",
      "  - Low: 36.01°C\n",
      "  - Feels like: 37.3°C\n",
      "Rain: {}\n",
      "Heat index: None\n",
      "Cloud cover: 0%\n"
     ]
    }
   ],
   "source": [
    "weather_data = weather.run(\"Las Vegas\")\n",
    "print(weather_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce2db9",
   "metadata": {},
   "source": [
    "이제 이를 function_2에 통합하고 workflow에서 \"node_2\" 대신 \"tool\" 또는 \"weather_agent\"로 function_2를 호출해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8819926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    complete_query = \"Your task is to provide only the city name based on the user query. \\\n",
    "    Nothing more, just the city name mentioned. Following is the user query: \" + input_1\n",
    "    response = model.invoke(complete_query)\n",
    "    return response.content\n",
    "\n",
    "def function_2(input_2):\n",
    "    weather_data = weather.run(input_2)\n",
    "    return weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98af5b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "workflow = Graph()\n",
    "workflow.add_node('agent', function_1)\n",
    "workflow.add_node('tool', function_2)\n",
    "workflow.add_edge('agent', 'tool')\n",
    "workflow.set_entry_point('agent')\n",
    "workflow.set_finish_point('tool')\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb081984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Las Vegas, the current weather is as follows:\\nDetailed status: clear sky\\nWind speed: 6.69 m/s, direction: 170°\\nHumidity: 23%\\nTemperature: \\n  - Current: 37.95°C\\n  - High: 39.69°C\\n  - Low: 36.01°C\\n  - Feels like: 37.3°C\\nRain: {}\\nHeat index: None\\nCloud cover: 0%'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\"What's the temperature in Las Vegas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4537f02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent': 'Las Vegas'\n",
      "====\n",
      "Output from node 'tool': 'In Las Vegas, the current weather is as follows:\n",
      "Detailed status: clear sky\n",
      "Wind speed: 6.69 m/s, direction: 170°\n",
      "Humidity: 23%\n",
      "Temperature: \n",
      "  - Current: 37.95°C\n",
      "  - High: 39.69°C\n",
      "  - Low: 36.01°C\n",
      "  - Feels like: 37.3°C\n",
      "Rain: {}\n",
      "Heat index: None\n",
      "Cloud cover: 0%'\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "input = \"What's the temperature in Las Vegas\"\n",
    "for output in app.stream(input):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}': '{value}'\")\n",
    "    print('====')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee0a60",
   "metadata": {},
   "source": [
    "#### Step 3 Adding another LLM Call to filter results\n",
    "온도만 원한다면 어떻게 해야할까요? 현재 설정에서는 전체 일기 예보를 제공합니다.\n",
    "\n",
    "데이터를 필터링하기 위해 또 다른 LLM 호출을 할 수 있습니다.\n",
    "\n",
    "<img src=\"langgraph-5.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b86a0eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_3(input_3):\n",
    "    complete_query = \"Your task is to provide info concisely based on the user query. Following is the user query: \" + \"user input\"\n",
    "    response = model.invoke(complete_query)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a112231",
   "metadata": {},
   "source": [
    "그러나 문제는 node_2에서 \"user_input\"을 알 수 없다는 것입니다.\n",
    "\n",
    "첫 번째 node 에서 마지막 node 까지 \"user_input\"을 전달할 수 있나요?\n",
    "\n",
    "예, dictionary 을 사용하여 node 간에 전달할 수 있습니다(list 만 사용할 수도 있지만, dictionary 를 사용하면 좀 더 쉽습니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "515c7d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign AgentState as an empty dict\n",
    "AgentState = {}\n",
    "\n",
    "# messages key will be assigned as an empty array. We will append new messages as we pass along nodes. \n",
    "AgentState[\"messages\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6cf54b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': []}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AgentState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34318f7d",
   "metadata": {},
   "source": [
    "우리의 목표는 이 상태(State)를 다음과 같이 채우는 것입니다: \n",
    "```\n",
    "{'messages': [HumanMessage, AIMessage, ...]]}\n",
    "```\n",
    "또한 이제 새 AgentState 에 따라 정보를 전달하도록 함수를 수정해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1c864a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(state):\n",
    "    messages = state['messages']\n",
    "    user_input = messages[-1]\n",
    "    complete_query = \"Your task is to provide only the city name based on the user query. \\\n",
    "                    Nothing more, just the city name mentioned. Following is the user query: \" + user_input\n",
    "    response = model.invoke(complete_query)\n",
    "    state['messages'].append(response.content) # appending AIMessage response to the AgentState\n",
    "    return state\n",
    "\n",
    "def function_2(state):\n",
    "    messages = state['messages']\n",
    "    agent_response = messages[-1]\n",
    "    weather = OpenWeatherMapAPIWrapper()\n",
    "    weather_data = weather.run(agent_response)\n",
    "    state['messages'].append(weather_data)\n",
    "    return state\n",
    "\n",
    "def function_3(state):\n",
    "    messages = state['messages']\n",
    "    user_input = messages[0]\n",
    "    available_info = messages[-1]\n",
    "    agent2_query = \"Your task is to provide info concisely based on the user query and the available information from the internet. \\\n",
    "                        Following is the user query: \" + user_input + \" Available information: \" + available_info\n",
    "    response = model.invoke(agent2_query)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43c1a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"tool\", function_2)\n",
    "workflow.add_node(\"responder\", function_3)\n",
    "\n",
    "workflow.add_edge('agent', 'tool')\n",
    "workflow.add_edge('tool', 'responder')\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.set_finish_point(\"responder\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "125ca4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature in Las Vegas is 36.7°C with clear skies and a low humidity of 27%.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"messages\": [\"what is the temperature in Las Vegas\"]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2cd87a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent': '{'messages': ['what is the temperature in  Las Vegas', 'Las Vegas']}'\n",
      "====\n",
      "Output from node 'tool': '{'messages': ['what is the temperature in  Las Vegas', 'Las Vegas', 'In Las Vegas, the current weather is as follows:\\nDetailed status: clear sky\\nWind speed: 1.34 m/s, direction: 267°\\nHumidity: 27%\\nTemperature: \\n  - Current: 36.7°C\\n  - High: 37.28°C\\n  - Low: 36.18°C\\n  - Feels like: 36.48°C\\nRain: {}\\nHeat index: None\\nCloud cover: 8%']}'\n",
      "====\n",
      "Output from node 'responder': 'The current temperature in Las Vegas is 36.7°C with clear skies, a low humidity of 27%, and a light wind speed of 1.34 m/s.'\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "input = {\"messages\": [\"what is the temperature in  Las Vegas\"]}\n",
    "for output in app.stream(input):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}': '{value}'\")\n",
    "    print(\"====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c11f4e0",
   "metadata": {},
   "source": [
    "array 에 많은 덧붙임(appending)이 진행되고 있음을 알 수 있으므로, 다음을 사용하여 좀 더 쉽게 만들 수 있습니다:\n",
    "\n",
    "```python\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "```\n",
    "기본적으로 이전에 본 것처럼 State dictionary 를 만들고, 다음을 수행할 때 새 메시지가 메시지 배열에 추가되는지 확인합니다.\n",
    "\n",
    "```python\n",
    "{\"messages\": [new_array_element]}\n",
    "```\n",
    "\n",
    "또한 우리 앱이 \"how are you?\"와 같은 간단한 질문에 답할 수 없다는 것도 알고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a93128e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am an AI assistant, so I don't have feelings, but thank you for asking. In Istanbul, the current weather is clear with a temperature of 22.89°C, a wind speed of 7.72 m/s, and 61% humidity.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"messages\": [\"How are you?\"]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86302e12",
   "metadata": {},
   "source": [
    "이는 우리가 항상 도시를 분석한 다음 날씨를 찾고 싶기 때문입니다.\n",
    "\n",
    "사용자에게 단순히 응답하는 것이 아니라, 필요한 경우에만 도구를 사용하도록 하여 에이전트를 더 똑똑하게 만들 수 있습니다.\n",
    "\n",
    "LangGraph를 수행할 수 있는 방법은 다음과 같습니다.\n",
    "\n",
    "1. tool 를 agent 에 바인딩\n",
    "2. tool 를 호출할지 여부를 선택할 수 있는 옵션으로 agent에 conditianal edge 를 추가합니다.\n",
    "3. tool 를 호출할 시기와 같이 conditianal edge에 대한 기준을 정의합니다. 이에 대한 함수를 정의하겠습니다.\n",
    "\n",
    "위의 몇 가지 셀에서 언급한 AgentState 정의부터 시작해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8f7b0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1c398f",
   "metadata": {},
   "source": [
    "tool을 에이전트(LLM 모델)에 바인딩하는 것이 langchain에서 쉽게 만들어집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f92da8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_community.tools.openweathermap import OpenWeatherMapQueryRun\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "tools = [OpenWeatherMapQueryRun()]\n",
    "\n",
    "model = ChatOpenAI(temperature=0, streaming=True)\n",
    "functions = [convert_to_openai_function(t) for t in tools]\n",
    "model = model.bind_functions(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18962da",
   "metadata": {},
   "source": [
    "수정된 function_1은 이제 아래와 같습니다. 그 이유는 인간의 메시지를 state로 전달하고, 응답을 state에 추가하기 때문입니다. 또한 이제 agent 에는 사용할 수 있는 tool 이 바인딩되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29286431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(state):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e7a61",
   "metadata": {},
   "source": [
    "function_2의 경우 tool 을 설정하고 호출하기를 원합니다. ToolInvocation을 사용하고, ToolExecuter로 실행하면 LangChain 에서 tool 을 쉽게 호출할 수 있습니다. 그런 다음 agent(node_1)가 tool 이 사용되었고, tool 의 응답이 가용하다는 것을 알 수 있도록, FunctionMessage 로 다시 응답합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d96138dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolInvocation\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "def function_2(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1] # this has the query we need to send to the tool provided by the agent\n",
    "\n",
    "    parsed_tool_input = json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"])\n",
    "\n",
    "    # We construct an ToolInvocation from the function_call and pass in the tool name and the expected str input for OpenWeatherMap tool\n",
    "    action = ToolInvocation(\n",
    "        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "        tool_input=parsed_tool_input['__arg1'],\n",
    "    )\n",
    "    \n",
    "    # We call the tool_executor and get back a response\n",
    "    response = tool_executor.invoke(action)\n",
    "\n",
    "    # We use the response to create a FunctionMessage\n",
    "    function_message = FunctionMessage(content=str(response), name=action.tool)\n",
    "\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [function_message]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e58bb4",
   "metadata": {},
   "source": [
    "마지막으로, 어떤 방향(tool 또는 사용자 응답)으로 가야 할지 파악하는 데 도움이 되는 conditional edge 에 대한 함수를 정의합니다.\n",
    "\n",
    "우리는 tool 이름으로 function_call을 만들기 위한 additional_kwargs 를 가지고 있는 LangChain의 agent(LLM) 응답으로부터 이점을 얻을 수 있습니다.\n",
    "\n",
    "따라서 우리의 논리는 additional_kwargs 에서 function_call을 사용할 수 있으면 tool 을 호출하고, 그렇지 않으면 논의를 종료하고 사용자에게 다시 응답하는 것입니다.\n",
    "\n",
    "<img src=\"langgraph-6.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9b78e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def where_to_go(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    if \"function_call\" in last_message.additional_kwargs:\n",
    "        return \"continue\"\n",
    "    else:\n",
    "        return \"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43782915",
   "metadata": {},
   "source": [
    "이제 위의 모든 변경 사항을 적용하여 LangGraph 앱이 아래와 같이 수정되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "be4d022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langgraph.graph import Graph, END\n",
    "\n",
    "# workflow = Graph()\n",
    "\n",
    "# Or you could import StateGraph and pass AgentState to it\n",
    "from langgraph.graph import StateGraph, END\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"tool\", function_2)\n",
    "\n",
    "# The conditional edge requires the following info below.\n",
    "# First, we define the start node. We use `agent`.\n",
    "# This means these are the edges taken after the `agent` node is called.\n",
    "# Next, we pass in the function that will determine which node is called next, in our case where_to_go().\n",
    "\n",
    "workflow.add_conditional_edges(\"agent\", where_to_go,{   # Based on the return from where_to_go\n",
    "                                                        # If return is \"continue\" then we call the tool node.\n",
    "                                                        \"continue\": \"tool\",\n",
    "                                                        # Otherwise we finish. END is a special node marking that the graph should finish.\n",
    "                                                        \"end\": END\n",
    "                                                    }\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that if `tool` is called, then it has to call the 'agent' next. \n",
    "workflow.add_edge('tool', 'agent')\n",
    "\n",
    "# Basically, agent node has the option to call a tool node based on a condition, \n",
    "# whereas tool node must call the agent in all cases based on this setup.\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04ce6ae",
   "metadata": {},
   "source": [
    "또한 langchain에서 사용할 수 있는 HumanMessage 구성 요소를 사용하여 첫 번째 메시지를 전달하므로 AIMessage 및 FunctionMessage와 쉽게 구별할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4c5cf444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the temperature in las vegas'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"__arg1\":\"Las Vegas\"}', 'name': 'open_weather_map'}}, response_metadata={'finish_reason': 'function_call'}, id='run-f0a8b6c2-4c14-4740-8f82-13656a0677e4-0'),\n",
       "  FunctionMessage(content='In Las Vegas, the current weather is as follows:\\nDetailed status: clear sky\\nWind speed: 4.63 m/s, direction: 180°\\nHumidity: 24%\\nTemperature: \\n  - Current: 37.24°C\\n  - High: 38.38°C\\n  - Low: 36.01°C\\n  - Feels like: 36.54°C\\nRain: {}\\nHeat index: None\\nCloud cover: 0%', name='open_weather_map'),\n",
       "  AIMessage(content='The current temperature in Las Vegas is 37.24°C.', response_metadata={'finish_reason': 'stop'}, id='run-57f56fb8-303f-4ce7-a169-669f732d643b-0')]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"what is the temperature in las vegas\")]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09ff6e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent': '{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"__arg1\":\"Las Vegas\"}', 'name': 'open_weather_map'}}, response_metadata={'finish_reason': 'function_call'}, id='run-4c894e29-d3a5-4d4b-828e-635762622960-0')]}'\n",
      "=======\n",
      "Output from node 'tool': '{'messages': [FunctionMessage(content='In Las Vegas, the current weather is as follows:\\nDetailed status: clear sky\\nWind speed: 4.63 m/s, direction: 180°\\nHumidity: 24%\\nTemperature: \\n  - Current: 37.24°C\\n  - High: 38.38°C\\n  - Low: 36.01°C\\n  - Feels like: 36.54°C\\nRain: {}\\nHeat index: None\\nCloud cover: 0%', name='open_weather_map')]}'\n",
      "=======\n",
      "Output from node 'agent': '{'messages': [AIMessage(content='The current temperature in Las Vegas is 37.24°C.', response_metadata={'finish_reason': 'stop'}, id='run-eada6116-159d-4c33-b304-f5f046aa91df-0')]}'\n",
      "=======\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"what is the temperature in las vegas\")]}\n",
    "for output in app.stream(inputs):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}': '{value}'\")\n",
    "    print(\"=======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "15e4133d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm just a computer program, so I don't have feelings, but I'm here to help you with any questions or tasks you have. How can I assist you today?\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"how are you?\")]}\n",
    "app.invoke(inputs)['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "37325dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature in Seoul is 30.74°C.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"what is the temperature in Seoul\")]}\n",
    "app.invoke(inputs)['messages'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f9fc27",
   "metadata": {},
   "source": [
    "이를 통해 우리가 LangGraph 앱을 구축한 방법과 다양한 LangChain 구성요소를 사용한 이유를 잘 이해할 수 있기를 바랍니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
